{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras 기초문법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 형성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy / pandas # 데이터셋 형성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras.model.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instance를 생성한다. 제일 먼저 이루어져야 하는 층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.add()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 층(layer)을 쌓아가면서 구성한다.<br>\n",
    "\n",
    "**keras.layers.Dense()** <br>\n",
    "\n",
    "fully connected layer 를 추가한다.\n",
    "\n",
    "- units(첫번째 인자) : <br>\n",
    "    맨 앞에 쓰이는 정수값으로서 output의 갯수를 의미 <Br>\n",
    "- input_dim : <br> \n",
    "    맨 앞의 layer 층에서 쓰인다. 입력데이터 X의 한 샘플에 대한 모양의 dim 을 적어주면 된다. <br>\n",
    "    주로 X_train.shape[1:] 를 쓴다. (X_train 의 row(데이터 수) 가 제일 앞에 나오고 그 뒤는 데이터의 형태이기 떄문)\n",
    "- activation : 활성화함수를 추가한다. <br>\n",
    "    - linear : 디폴트값. 활성화 함수 없이 그냥 뉴런과 가중치의 계산 결과 그대로 출력\n",
    "    - sigmoid : 주로 이진분류문제의 맨 마지막 출력층에 사용 \n",
    "    - softmax : 주로 다중클래스분류의 맨 마지막 출력층에 사용되어 확률을 출력\n",
    "    - relu : hidden layer 에 주로 사용되는 활성화함수<br>\n",
    "- kernel_initializer=\"he_normal\"<br>\n",
    "    초기에 연결 가중치를 어떻게 초기화할지의 방법<br>\n",
    "    kernel initializer 가 초기화하는 부분은 노드 전에 이어져있는 부분이다. -(이부분의 w)- node\n",
    "    - he_normal/he_uniform : HE 초기화. Relu 함수와 그 변종(LeakyRelu 등) 을 위한 초기화방법<br> \n",
    "    - GlorotNormal/GlorotUniform : 글로럿 초기화. 활성화함수가 없거나, tanh/sigmoid/softmax 를 위한 초기화방법<br>\n",
    "    - lecun_normal : 르쿤 초기화. SELU 를 위한 초기화방법<br>\n",
    "     \n",
    " \n",
    "**keras.layers.Flatten()**\n",
    "\n",
    "input 에 대해서 1dim 의 벡터 형태의 데이터로 변환해준다.\n",
    "   \n",
    "- input_shape=[]<br>\n",
    "    input으로 받는 데이터의 모양 (ex [28,28])\n",
    "    \n",
    "**keras.layers.BatchNormalization()**\n",
    "\n",
    "배치 정규화.  입력값에 대해서 정규화를 통해 스케일과 평균을 조절해 데이터의 분포를 비슷하게 맞춰준다. <br>\n",
    "주로 활성화 함수를 통과하기 전에 실행한다.<br>\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실함수 정의 등 학습과정 설정\n",
    "\n",
    "**optimizer**\n",
    "\n",
    "최적화 방법\n",
    "\n",
    "- sgd      = keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
    "- momentum = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "- Adagrad  = keras.optimizers.Adagrad(lr=0.001)\n",
    "- RMSprop  = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n",
    "- Adam     = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "- Adamax   = keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "- Nadam    = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)<br>\n",
    "@@ 이때 위에 decay(학습률이 얼마나 줄어드는지) 를 추가할 수 있다.\n",
    "\n",
    "\n",
    "**loss**\n",
    "\n",
    "손실함수. 모델 fitting 시 이걸 기준으로 최적화\n",
    " - mse : mean squared error\n",
    " - binary_crossentropy <br>\n",
    "     : 이진분류를 할 떄에 corss entropy계산 <br>\n",
    "     : 이진분류시 출력층 : sigmoid + 손실함수 binary cross entropy 사용\n",
    " - categorical_crossentropy <br>\n",
    "     : y가 one hot incoding 이 되어있는 경우 cross entropy를 계산 (target과 output의 shape가 같아야 함) <br>\n",
    "     : softmax 출력층과 같이 사용된다.\n",
    " - sparse_categorical_crossentropy <br>\n",
    "     : y가 ont hot incoding 이 안되있는경우 ex1~10 의 값을 가지는 1dim vector corss entropy 계산 <br>\n",
    "     : softmax 출력층과 같이 사용된다.\n",
    "\n",
    "**metrics**\n",
    "\n",
    "모델의 평가 기준 \n",
    " - accuracy (Calculates how often predictions equals labels) (회귀떄에는 사용불가. 의미가없음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델학습\n",
    "\n",
    "**첫번째 인자**\n",
    "\n",
    "X_train\n",
    "\n",
    "**두번째 인자**\n",
    "\n",
    "y_train\n",
    "\n",
    "**batch_size = 32(default)**<br>\n",
    "   \n",
    "학습할떄 몇개의 Sample 을 묶어서 update 할떄에 적용할지\n",
    " \n",
    "**epochs =5**<br>\n",
    "\n",
    "총 데이터를 몇번 반복해서 학습할지\n",
    "       \n",
    "**shuffle =True**<br>\n",
    "\n",
    "학습하면서 set 을 random 하게 섞을지\n",
    "        \n",
    "**verbose = 1(default)**<br>\n",
    "\n",
    "학습이 이루어지는 과정을 직접 볼지 안볼지 (default = 1 로 과정을 볼 수 있다.)\n",
    "\n",
    "**validation_data(X_val,y_val)**\n",
    "\n",
    "검증데이터를 사용한다.<br>\n",
    "검증데이터 사용시 각 에포크 마다 검증데이터의 정확도도 같이 출력이 되는데, 이 경우 훈련이 잘 되고있는지를 보여줄 뿐이며 실제로 모델이 검증데이터를 학습하지는 않는다. <br>\n",
    "검증데이터의 loss 가 낮아지다가 높아지기 시작하면 이는 overfitting \n",
    " \n",
    "**validation_split = 0.2**<br>\n",
    "     \n",
    "검증데이터를 사용하는것은 동일하지만 , 별도로 존재하는 검증데이터를 주는게 아니라 , X_train과 y_train에서 일정 비율을 분리하여 이를 검증 데이터로 사용한다.<br>\n",
    "역시나 훈련 자체에는 반영되지 않고 훈련 과정을 지켜보기 위한 용도로 사용. validation_data 대신 사용 가능하다.\n",
    "\n",
    "**callback = [checkpoint,earlystopping,tensorboard,lr_scheduler]**\n",
    "\n",
    "이 부분은 아래 값들이 가능하다.\n",
    "\n",
    "**1.keras.callbacks.ModelCheckpoint()** <br>\n",
    "\n",
    "모델이 기준에 따라 제일 좋을떄에 저장하는 옵션\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint() <br>\n",
    "model.fit(callbacks=[checkpoint]) # 처럼 사용 <br>\n",
    "- filepath<br>\n",
    "    문자열. 모델 파일을 저장할 경로 <br>\n",
    "- monitor='val_loss' <br>\n",
    "    어떤것을 기준으로 모델을 저장할지 <br>\n",
    "- verbose=0, <br>\n",
    "- save_best_only=False <br>\n",
    "    save_best_only는 모델의 정확도가 최고값을 갱신했을 때만 저장하도록 하는 옵션 <br>\n",
    "- save_weights_only=False <br>\n",
    "    true 면 모델의 가중치만 저장된다. <br>\n",
    "- mode='auto'<br>\n",
    "    svae_best_only True 일 경우 <br>\n",
    "    min : 우리가 정한 기준의 최소값보다 낮을떄 저장 덧씌우기(loss)<br>\n",
    "    max : 우리가 정한 기준의 최대값보다 높을떄 모델 덧씌워서 저장(acc)<br>\n",
    "    auto : 우리가 정한 loss 에 따라 자동유추(추천!)<br> \n",
    "- period=1<br>\n",
    "    체크포인트 간격 <br>\n",
    "\n",
    "**2.keras.callbacks.EarlyStopping()** <br>\n",
    "\n",
    "모델이 기준에 따라 더 학습되지 않는다고 판단하면 멈추는 옵션\n",
    "\n",
    "earlystop = keras.callbacks.ModelEarlyStopping() <br>\n",
    "history = model.fit(callbacks=[earlystop]) # fitting 시에 EarlyStopping 이 같이 들어간다.\n",
    "- monitor='val_loss'<br>\n",
    "    어떤것을 기준으로 학습을 stop 할지 결정\n",
    "- min_delta=0 <br>\n",
    "    우리가 정한 기준을 바탕으로 모델이 향상되었다고 판단할 최소한의 변화. min delta 미만의 절대적 변화는 향상되었다고 판단하지 않는다.\n",
    "- patience=0<br>\n",
    "    개선이 없다고 바로 종료하지 않고, 인내를 가지고 기다려주는 값으로 epoch 를 몇번이나 더 허용할지 결정\n",
    "- verbose=0 <br>\n",
    "- mode='auto'<br>\n",
    "    min : 정한 기준값이 더이상 감소하지 않으면 학습이 멈춘다.(loss 와 사용)<br>\n",
    "    max : 정한 기준값이 더이상 증가하지 않으면 학습이 멈춘다.(acc 와 사용)<br>\n",
    "    auto : 우리가 정한 기준값에서 방향성이 자동으로 유추된다. <br>\n",
    "- baseline=None <br>\n",
    "    관찰하는 수량이 도달해야하는 베이스라인 값 모델이 베이스라인을 초과하는 향상을 보이지 않으면 학습이 멈춘다.\n",
    "- restore_best_weights=False <br>\n",
    "\n",
    "**3.keras.callbacks.TensorBoard()**\n",
    "\n",
    "**4.keras.callbacks.ReduceLROnPlateau()**\n",
    "\n",
    "학습률을 점점 줄여나가는 스케일링 옵션\n",
    "\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)<br>\n",
    "history = model.fit(callbacks=[lr_scheduler])\n",
    "\n",
    "- factor = 0.1<br>\n",
    "    모델이 향상이 없으면 learning rate 를 얼마나 줄일지\n",
    "- patience = 10<br>\n",
    "    모델이 향상이 없을때 얼마나 더 지켜볼지\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model.evaluate # 모델 평가하기**\n",
    "테스트 데이터를 통해 학습한 모델에 대한 정확도를 평가한다.\n",
    "\n",
    "- 첫번째 인자 = X_test\n",
    "\n",
    "- 두번쨰 인자 = y_test\n",
    "\n",
    "- batch_size :<br>\n",
    "    왜 evaluate 에 batch size 가 필요할까? 그건 계산속도를 빠르게 하기 위함이다. 묶어서 계산할뿐이라 값은 거의 차이 안난다.<br>\n",
    "    The evaluate function of Model has a batch size just in order to speed-up evaluation, as the network can process multiple samples at a time, and with a GPU this makes evaluation much faster. I think the only way to reduce the effect of this would be to set batch_size to one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model.predict() # 모델 예측하기**\n",
    "\n",
    "- 첫번째 인자 : 예측하고자 하는 데이터 (X_input)\n",
    "- 가장 높은 클래스에만 관심이 있으면 model.predict_classes 를 쓴다.\n",
    "    - y_pred = model.predict_classes(X_new)<br>\n",
    "      y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "history = model.fit(X,y) 으로 fitting 해서 model 을 형성했다고 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### history.history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fit.history**\n",
    "\n",
    "fit = model.fit() 으로 학습시키고 나면, 이제 이 fit 에는 우리가 학습시킬때의 과정중에서 loss. accuracy(모델의 성능평가 metric 에서 지정한 기준으로) 가 어떻게 변화하였는지를 알려주는 dictionary 가 형성된다. key와 value 를 이용해서 그래프를 그려서 학습과정을 시각화할 수 있을것이다.<br>\n",
    "- pd.DataFrame(fit.history).plot<br>\n",
    "    - 이 명령어를 사용하면 각 epoch 별 loss,accuracy 의 변화를 살펴볼 수 있다. <br>\n",
    "    - 이떄에 만일 fit에서 validation data 도 설정하였다면 이 validation set 에서의 loss/ accuracy 또한 같이 산출해준다. <br>\n",
    "    - 그래프를 보면 초기에 validation에서의 accuracy/loss가 train 에 비해 더 낮아보인다. 이는 있을 수 없는일인데(fitting 이 train 에 대해서 일어나므로) 왜 이런현상이 일어나냐면, validation 의 accuracy/loss 는 epoch 가 끝난 후 측정되고, train 의 accuracy/loss 는 훈련도중에 형성이 되기떄문에, validation 의 경우는 좀 더 완벽한 모델에 대해서 평가가 되기 떄문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model.summary()**<br>\n",
    "우리가 정의한 모델의 구성요소/ 구조 등을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model.layers**<br>\n",
    "우리가 쌓아올린 model 들의 layer 들을 살펴볼 수 있다. <br>\n",
    "- hidden1 = model.layers[1] <br>\n",
    "    weights, biases = hidden1.get_weights() <br>\n",
    "\n",
    "처럼 layers 의 개별 layer를 하나만 떼어놓은 뒤에 그 layer 의 weights/ biases 값이 구체적으로 어떤값을 가지는지 조사할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장 및 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit() 으로 모델을 fitting 했다고 하자. 그러면<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model.save(\"저장할모델.h5\")**<br>\n",
    "모델을 쥬피터노트북과 같은 경로에 h5 확장자를 가지게 저장한다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model = keras.models.load_model(\"저장할모델.h5\")**<br>\n",
    "저장한 모델을 불러온다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
